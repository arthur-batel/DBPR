{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DBPR experiments\n",
    "### 1. Init\n",
    "#### 1.1. Import libraries"
   ],
   "id": "bc0774a07482a44f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:01:00.950362Z",
     "start_time": "2025-01-28T15:01:00.882660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "from DBPR import utils\n",
    "utils.set_seed(0)\n",
    "from DBPR import dataset\n",
    "from DBPR import model\n",
    "import optuna\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from importlib import reload"
   ],
   "id": "30c2bf011793765d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "CUDA is not available. Skipping CUDA seed setting.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.2. Start tensorboard",
   "id": "6f4d550fa43938b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:01:04.844061Z",
     "start_time": "2025-01-28T15:01:04.820521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorboard import notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --reuse=False --logdir /home/arthurb/Programmation/liriscat/experiments/tensorboard --load_fast=false --reload_interval=1 \n",
    "\n",
    "print(notebook.list())\n",
    "# access tensorboard at : http://localhost:6006"
   ],
   "id": "db4f414458219d16",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorboard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m notebook\n\u001B[1;32m      2\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload_ext\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensorboard\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensorboard\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--reuse=False --logdir /home/arthurb/Programmation/liriscat/experiments/tensorboard --load_fast=false --reload_interval=1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.3. Set up the loggers",
   "id": "18c248901436fb79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:01:44.807947Z",
     "start_time": "2025-01-28T15:01:44.786992Z"
    }
   },
   "cell_type": "code",
   "source": "utils.setuplogger(verbose = True, log_name=\"DBPR_postcovid\")",
   "id": "f544e590b910dcad",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.5. Parametrize the datasets",
   "id": "61158521634c09ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:01:46.662537Z",
     "start_time": "2025-01-28T15:01:46.643644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# choose dataset here\n",
    "dataset_name = 'postcovid'\n",
    "version= \"\"#\"_small\"\n",
    "# modify config here\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "config = {\n",
    "    \n",
    "    # General params\n",
    "    'seed' : 0,\n",
    "    \n",
    "    # Saving params\n",
    "    'load_params': False,\n",
    "    'save_params': False,\n",
    "    'embs_path' : '../embs/'+str(dataset_name),\n",
    "    'params_path' :'../ckpt/'+str(dataset_name),\n",
    "    \n",
    "    # training mode\n",
    "    'early_stopping' : True, \n",
    "    'fast_training' : True, # (Only taken in account if early_stopping == true) If true, doesn't compute valid rmse PC-ER\n",
    "    \n",
    "    # Learning params\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 2048,\n",
    "    'num_epochs': 200,\n",
    "    'num_dim': 10, # for IRT or MIRT todo : is it necessary as we use concepts knowledge number as embedding dimension ?\n",
    "    'eval_freq' : 1,\n",
    "    'patience' : 30,\n",
    "    'device': device,\n",
    "    'lambda' : 7.7e-6,\n",
    "    'tensorboard': False,\n",
    "    'flush_freq' : True,\n",
    "    \n",
    "    # for NeuralCD\n",
    "    'prednet_len1': 128,\n",
    "    'prednet_len2': 64,\n",
    "    'best_params_path':'',\n",
    "    \n",
    "    #For GCCD\n",
    "    'num_layers': 0,\n",
    "    'version': 'pair',\n",
    "    'p_dropout': 0,\n",
    "    'low_mem_mode' : True,\n",
    "    'user_nbrs_n' : 10,\n",
    "    'item_nbrs_n' : 5\n",
    "}\n",
    "concept_map = json.load(open(f'../datasets/{dataset_name}/concept_map.json', 'r'))\n",
    "concept_map = {int(k):[int(x) for x in v] for k,v in concept_map.items()}\n",
    "metadata = json.load(open(f'../datasets/{dataset_name}/metadata.json', 'r'))\n",
    "utils.set_seed(config['seed'])\n",
    "dataset_name += version\n",
    "logging.info(f'#### {dataset_name} ####')\n",
    "logging.info(f'#### config : {config} ####')"
   ],
   "id": "c4664f14b5fe9ad1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Skipping CUDA seed setting.\n",
      "[INFO 01:46] #### postcovid ####\n",
      "[INFO 01:46] #### config : {'seed': 0, 'load_params': False, 'save_params': False, 'embs_path': '../embs/postcovid', 'params_path': '../ckpt/postcovid', 'early_stopping': True, 'fast_training': True, 'learning_rate': 0.001, 'batch_size': 2048, 'num_epochs': 200, 'num_dim': 10, 'eval_freq': 1, 'patience': 30, 'device': 'cuda:0', 'lambda': 7.7e-06, 'tensorboard': False, 'flush_freq': True, 'prednet_len1': 128, 'prednet_len2': 64, 'best_params_path': '', 'num_layers': 0, 'version': 'pair', 'p_dropout': 0, 'low_mem_mode': True, 'user_nbrs_n': 10, 'item_nbrs_n': 5} ####\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. CDM Hyperparameter search",
   "id": "2eecb5813a4dcd9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.1. Sequential",
   "id": "7f8a6aadbcd3a357"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "reload(utils)\n",
    "reload(model)\n",
    "reload(dataset)\n",
    "\n",
    "\n",
    "seed = 0\n",
    "utils.set_seed(0)\n",
    "\n",
    "config['seed'] = seed\n",
    "config['early_stopping'] = True\n",
    "config['esc'] = 'error'#'objectives' #'loss' 'delta_objectives'\n",
    "config['num_epochs']=200\n",
    "config['eval_freq']=1\n",
    "config['patience']=30\n",
    "\n",
    "config['verbose_early_stopping'] = False\n",
    "config[\"tensorboard\"] = False\n",
    "config['flush_freq'] = False\n",
    "config['save_params']= False\n",
    "config['disable_tqdm'] = True\n",
    "\n",
    "\n",
    "    \n",
    "def load_dataset(dataset_name : str) :\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # read datasets\n",
    "    i_fold = 0\n",
    "    concept_map = json.load(open(f'../datasets/{dataset_name}/concept_map.json', 'r'))\n",
    "    concept_map = {int(k):[int(x) for x in v] for k,v in concept_map.items()}\n",
    "    metadata = json.load(open(f'../datasets/{dataset_name}/metadata.json', 'r'))\n",
    "    train_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_train_quadruples_vert_{i_fold}.csv',\n",
    "                             encoding='utf-8').to_records(index=False,\n",
    "                                                          column_dtypes={'student_id': int, 'item_id': int,\"dimension_id\":int,\n",
    "                                                                         \"correct\": float,\"dimension_id\":int})\n",
    "    valid_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_valid_quadruples_vert_{i_fold}.csv',\n",
    "                                 encoding='utf-8').to_records(index=False,\n",
    "                                                              column_dtypes={'student_id': int, 'item_id': int,\"dimension_id\":int,\n",
    "                                                                             \"correct\": float,\"dimension_id\":int})\n",
    "    \n",
    "    train_data = dataset.LoaderDataset(train_quadruplets, concept_map, metadata)\n",
    "    valid_data = dataset.LoaderDataset(valid_quadruplets, concept_map, metadata)\n",
    "    \n",
    "    return train_data,valid_data,concept_map,metadata\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 5e-2, log=True)\n",
    "    lambda_param = trial.suggest_float('lambda', 1e-7, 5e-4, log=True)\n",
    "    d_in =  trial.suggest_int('d_in', 3,5)\n",
    "    num_responses = trial.suggest_int('num_responses', 9,13)\n",
    "    \n",
    "    config['learning_rate'] = lr\n",
    "    config['lambda'] = lambda_param\n",
    "    config['d_in'] =d_in\n",
    "    config['num_responses'] =num_responses\n",
    "    \n",
    "    algo = model.DBPR(**config)\n",
    "        \n",
    "    # Init model\n",
    "    algo.init_model(train_data, valid_data)\n",
    "\n",
    "    # train model ----\n",
    "    algo.train(train_data, valid_data)\n",
    "    \n",
    "    best_valid_rmse = algo.best_valid_rmse\n",
    "    \n",
    "    logging.info(\"-------Trial number : \"+str(trial.number)+\"\\nBest epoch : \"+str(algo.best_epoch)+\"\\nValues : [\"+str(best_valid_rmse)+\"]\\nParams : \"+str(trial.params))\n",
    "    \n",
    "    del algo.model\n",
    "    del algo   \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "                \n",
    "    return best_valid_rmse"
   ],
   "id": "53c3976f69491bab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset_name = \"postcovid\"\n",
    "logging.info(dataset_name)\n",
    "train_data,valid_data,concept_map,metadata = load_dataset(dataset_name)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\"],  # Specify directions for each objective\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "study.optimize(objective, n_trials=2, n_jobs=1, gc_after_trial=True)\n",
    "\n",
    "# Analyze the results\n",
    "## requirements : plotly, nbformat\n",
    "pareto_trials = study.best_trials\n",
    "\n",
    "logging.info(f\"Best trial for {dataset_name} : {study.best_trials}\")\n",
    "for trial in pareto_trials:\n",
    "    logging.info(f\"Trial #{trial.number}\")\n",
    "    logging.info(f\"  RMSE: {trial.values}\")\n",
    "    #logging.info(f\"  DOA: {trial.values[1]}\")\n",
    "    logging.info(f\"  Params: {trial.params}\")"
   ],
   "id": "b7f525cc9a1b247f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.2. Parallelized",
   "id": "62dd8fde44ffb96d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:03:54.424501Z",
     "start_time": "2025-01-28T15:03:43.987520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#ipcluster start --n=3\n",
    "#ipcluster stop\n",
    "\n",
    "reload(utils)\n",
    "reload(model)\n",
    "reload(dataset)\n",
    "from ipyparallel import Client\n",
    "import dill\n",
    "\n",
    "cat_absolute_path = os.path.abspath('../../')\n",
    "\n",
    "rc = Client()\n",
    "rc[:].use_dill()\n",
    "lview = rc.load_balanced_view()\n",
    "\n",
    "\n",
    "rc[:].execute(\"import sys; sys.path.append('\"+cat_absolute_path+\"')\")\n",
    "print(\"sys.path.append(\"+cat_absolute_path+\")\")\n",
    "with rc[:].sync_imports():\n",
    "    import json\n",
    "    from DBPR import utils, model, dataset\n",
    "    import logging\n",
    "    import gc\n",
    "    import torch\n",
    "\n",
    "seed = 0\n",
    "utils.set_seed(0)\n",
    "\n",
    "config['seed'] = seed\n",
    "config['early_stopping'] = True\n",
    "config['esc'] = 'error'#'objectives' #'loss' 'delta_objectives'\n",
    "config['num_epochs']=200\n",
    "config['eval_freq']=1\n",
    "config['patience']=30\n",
    "\n",
    "config['verbose_early_stopping'] = False\n",
    "config[\"tensorboard\"] = False\n",
    "config['flush_freq'] = False\n",
    "config['save_params']= False\n",
    "config['disable_tqdm'] = True\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(dataset_name : str) :\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # read datasets\n",
    "    i_fold = 0\n",
    "    concept_map = json.load(open(f'../datasets/{dataset_name}/concept_map.json', 'r'))\n",
    "    concept_map = {int(k):[int(x) for x in v] for k,v in concept_map.items()}\n",
    "    metadata = json.load(open(f'../datasets/{dataset_name}/metadata.json', 'r'))\n",
    "    train_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_train_quadruples_vert_{i_fold}.csv',\n",
    "                             encoding='utf-8').to_records(index=False,\n",
    "                                                          column_dtypes={'student_id': int, 'item_id': int,\"dimension_id\":int,\n",
    "                                                                         \"correct\": float,\"dimension_id\":int})\n",
    "    valid_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_valid_quadruples_vert_{i_fold}.csv',\n",
    "                                 encoding='utf-8').to_records(index=False,\n",
    "                                                              column_dtypes={'student_id': int, 'item_id': int,\"dimension_id\":int,\n",
    "                                                                             \"correct\": float,\"dimension_id\":int})\n",
    "\n",
    "    train_data = dataset.LoaderDataset(train_quadruplets, concept_map, metadata)\n",
    "    valid_data = dataset.LoaderDataset(valid_quadruplets, concept_map, metadata)\n",
    "\n",
    "    return train_data,valid_data,concept_map,metadata\n",
    "\n",
    "def launch_test(trial,train_data,valid_data,config) :\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    algo = model.DBPR(**config)\n",
    "\n",
    "    # Init model\n",
    "    algo.init_model(train_data, valid_data)\n",
    "\n",
    "    # train model ----\n",
    "    algo.train(train_data, valid_data)\n",
    "\n",
    "    best_valid_rmse = algo.best_valid_rmse\n",
    "\n",
    "    logging.info(\"-------Trial number : \"+str(trial.number)+\"\\nBest epoch : \"+str(algo.best_epoch)+\"\\nValues : [\"+str(best_valid_rmse)+\"]\\nParams : \"+str(trial.params))\n",
    "\n",
    "    del algo.model\n",
    "    del algo\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return best_valid_rmse\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    lr = trial.suggest_float('learning_rate', 0.001, 0.01, log=True)\n",
    "    lambda_param = trial.suggest_float('lambda', 1.2e-6, 1.6e-6, log=True)\n",
    "    d_in =  trial.suggest_int('d_in', 5,7)\n",
    "    num_responses = trial.suggest_int('num_responses', 11,13)\n",
    "\n",
    "    config['learning_rate'] = lr\n",
    "    config['lambda'] = lambda_param\n",
    "    config['d_in'] =d_in\n",
    "    config['num_responses'] =num_responses\n",
    "\n",
    "    return lview.apply_async(launch_test,trial,train_data,valid_data, config).get()\n",
    "\n"
   ],
   "id": "1c938ca9da1d7b10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for connection file: ~/.ipython/profile_default/security/ipcontroller-client.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Connection file '~/.ipython/profile_default/security/ipcontroller-client.json' not found.\nYou have attempted to connect to an IPython Cluster but no Controller could be found.\nPlease double-check your configuration and ensure that a cluster is running.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 12\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdill\u001B[39;00m\n\u001B[1;32m     10\u001B[0m cat_absolute_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m rc \u001B[38;5;241m=\u001B[39m Client()\n\u001B[1;32m     13\u001B[0m rc[:]\u001B[38;5;241m.\u001B[39muse_dill()\n\u001B[1;32m     14\u001B[0m lview \u001B[38;5;241m=\u001B[39m rc\u001B[38;5;241m.\u001B[39mload_balanced_view()\n",
      "File \u001B[0;32m~/anaconda3/envs/cdbpr-env/lib/python3.11/site-packages/ipyparallel/client/client.py:456\u001B[0m, in \u001B[0;36mClient.__init__\u001B[0;34m(self, connection_info, url_file, profile, profile_dir, ipython_dir, context, debug, sshserver, sshkey, password, paramiko, timeout, cluster_id, cluster, **extra_args)\u001B[0m\n\u001B[1;32m    454\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(connection_file):\n\u001B[1;32m    455\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mshort\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m not found.\u001B[39m\u001B[38;5;124m\"\u001B[39m, no_file_msg])\n\u001B[0;32m--> 456\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(connection_file) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    459\u001B[0m     connection_info \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n",
      "\u001B[0;31mOSError\u001B[0m: Connection file '~/.ipython/profile_default/security/ipcontroller-client.json' not found.\nYou have attempted to connect to an IPython Cluster but no Controller could be found.\nPlease double-check your configuration and ensure that a cluster is running."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%time\n",
    "dataset_name = \"movielens\"\n",
    "logging.info(dataset_name)\n",
    "train_data,valid_data,concept_map,metadata = load_dataset(dataset_name)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\"],  # Specify directions for each objective\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "study.optimize(objective, n_trials=100, n_jobs=3, gc_after_trial=True)\n",
    "\n",
    "# Analyze the results\n",
    "## requirements : plotly, nbformat\n",
    "pareto_trials = study.best_trials\n",
    "\n",
    "logging.info(f\"Best trial for {dataset_name} : {study.best_trials}\")\n",
    "for trial in pareto_trials:\n",
    "    logging.info(f\"Trial #{trial.number}\")\n",
    "    logging.info(f\"  RMSE: {trial.values}\")\n",
    "    #logging.info(f\"  DOA: {trial.values[1]}\")\n",
    "    logging.info(f\"  Params: {trial.params}\")"
   ],
   "id": "2903754279c264b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.3. Number of parameters computation",
   "id": "da61a9e00a0211e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "d_in=5\n",
    "num_responses=13\n",
    "metadata['num_item_id']*num_responses*d_in+metadata['num_user_id']*metadata['num_dimension_id']+metadata['num_dimension_id']*metadata['num_dimension_id']*d_in"
   ],
   "id": "c1ba9c0a898796ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. CDM Prediction\n",
    "#### 3.1. Parallel training and testing"
   ],
   "id": "6d574559bd418d77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#ipcluster start --n=4\n",
    "#ipcluster stop"
   ],
   "id": "303c944f807c2383"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:04:27.789290Z",
     "start_time": "2025-01-28T15:04:18.654298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reload(utils)\n",
    "reload(model)\n",
    "reload(dataset)\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from ipyparallel import Client\n",
    "from DBPR import utils\n",
    "import dill\n",
    "\n",
    "cat_absolute_path = os.path.abspath('../../')\n",
    "\n",
    "rc = Client()\n",
    "rc[:].use_dill()\n",
    "lview = rc.load_balanced_view()\n",
    "\n",
    "# Synchronize imports with all engines:\n",
    "rc[:].execute(\"import sys; sys.path.append('\"+cat_absolute_path+\"')\")\n",
    "rc[:].execute(\"import os; os.chdir('./liriscat/experiments/notebook_examples')\")\n",
    "\n",
    "with rc[:].sync_imports():\n",
    "    from DBPR import utils, model, dataset\n",
    "\n",
    "config[\"disable_tqdm\"] = True\n",
    "config[\"tensorboard\"] = False\n",
    "config['flush_freq'] = False\n",
    "config['early_stopping'] = True\n",
    "config['save_params']=True # Save all model parameters and save an array of the embeddings\n",
    "config['verbose_early_stopping'] = False\n",
    "config['esc'] = 'error'\n",
    "\n",
    "def launch_training(seed:int,config:dict,train_data,valid_data,test_data, concept_map) :\n",
    "    utils.set_seed(seed)\n",
    "    config['seed'] = seed\n",
    "\n",
    "    algo = model.DBPR(**config)\n",
    "\n",
    "    # Init model\n",
    "    algo.init_model(train_data, valid_data)\n",
    "\n",
    "    # train model ----\n",
    "    algo.train(train_data, valid_data)\n",
    "\n",
    "    emb = algo.model.users_emb.weight.detach().cpu().numpy()\n",
    "\n",
    "    metrics = {\"mae\":[],\"rmse\":[], \"r2\":[], \"pc-er\" : []}\n",
    "\n",
    "    metrics[\"pc-er\"].append(algo.evaluate_emb(test_data,concept_map)['pc-er'])\n",
    "    eval = algo.evaluate_test(test_data)\n",
    "    metrics[\"rmse\"].append(eval[\"rmse\"].numpy().tolist())\n",
    "    metrics[\"mae\"].append(eval[\"mae\"].numpy().tolist())\n",
    "    metrics[\"r2\"].append(eval[\"r2\"].numpy().tolist())\n",
    "\n",
    "    return (metrics,emb)\n",
    "\n",
    "def fold_test(i_fold : int, dataset_name:str, config : dict) :\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Dataset downloading for doa and rm\n",
    "    warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in divide\")\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    concept_map = json.load(open(f'../datasets/{dataset_name}/concept_map.json', 'r'))\n",
    "    concept_map = {int(k): [int(x) for x in v] for k, v in concept_map.items()}\n",
    "    metadata = json.load(open(f'../datasets/{dataset_name}/metadata.json', 'r'))\n",
    "    concept_array, concept_lens=utils.preprocess_concept_map(concept_map)\n",
    "\n",
    "    # read datasets\n",
    "    train_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_train_quadruples_vert_{i_fold}.csv',\n",
    "                                 encoding='utf-8').to_records(index=False,\n",
    "                                                              column_dtypes={'student_id': int, 'item_id': int,\n",
    "                                                                             \"correct\": float,\"dimension_id\":int})\n",
    "    valid_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_valid_quadruples_vert_{i_fold}.csv',\n",
    "                                 encoding='utf-8').to_records(index=False,\n",
    "                                                              column_dtypes={'student_id': int, 'item_id': int,\n",
    "                                                                             \"correct\": float,\"dimension_id\":int})\n",
    "    test_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_test_quadruples_vert_{i_fold}.csv',\n",
    "                                encoding='utf-8').to_records(index=False,\n",
    "                                                             column_dtypes={'student_id': int, 'item_id': int,\n",
    "                                                                            \"correct\": float,\"dimension_id\":int})\n",
    "\n",
    "    train_data = dataset.LoaderDataset(train_quadruplets, concept_map, metadata)\n",
    "    valid_data = dataset.LoaderDataset(valid_quadruplets, concept_map, metadata)\n",
    "    test_data = dataset.LoaderDataset(test_quadruplets, concept_map, metadata)\n",
    "\n",
    "    seeds_combinations = []\n",
    "    for seed in range(3) :\n",
    "        seeds_combinations.append((seed,lview.apply_async(launch_training,seed,config,train_data,valid_data,test_data, concept_map)))\n",
    "\n",
    "    metrics = {\"mae\":[],\"rmse\":[], \"r2\":[], \"pc-er\" : [], \"doa\": [], 'rm' : []}\n",
    "\n",
    "    for seed,async_result in seeds_combinations:\n",
    "        metric, emb = async_result.get()\n",
    "        # test model ----\n",
    "        logging.info(f\"Test done - seed : {seed}, i_fold : {i_fold}\")\n",
    "        for k in metric.keys():\n",
    "            metrics[k].extend(metric[k])\n",
    "\n",
    "        metrics[\"doa\"].append(np.mean(utils.evaluate_doa(emb,test_data.log_tensor.cpu().numpy(),metadata,concept_map)))\n",
    "        metrics[\"rm\"].append(np.mean(utils.compute_rm_fold(emb,test_quadruplets, concept_array, concept_lens)))\n",
    "        pd.DataFrame(emb).to_csv(\"../embs/\"+dataset_name+\"_DBPR_cornac_Iter_fold\"+str(i_fold)+\"_seed_\"+str(seed)+\".csv\",index=False,header=False)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def test(dataset_name:str, config : dict) :\n",
    "\n",
    "    logging.info(f'#### {dataset_name} ####')\n",
    "    logging.info(f'#### config : {config} ####')\n",
    "\n",
    "    metrics = {\"mae\":[],\"rmse\":[], \"r2\":[], \"pc-er\" : [], \"doa\": [], 'rm' : []}\n",
    "\n",
    "    fold_combinations = []\n",
    "    for i_fold in range(5):\n",
    "        fold_combinations.append(fold_test(i_fold,dataset_name, config))\n",
    "\n",
    "    for metric in fold_combinations:\n",
    "        for k in metrics.keys():\n",
    "            metrics[k].extend(metric[k])\n",
    "\n",
    "    df = pd.DataFrame(metrics)\n",
    "    logging.info('rmse : {:.4f} +- {:.4f}'.format(df['rmse'].mean(), df['rmse'].std()))\n",
    "    logging.info('mae : {:.4f} +- {:.4f}'.format(df['mae'].mean(), df['mae'].std()))\n",
    "    logging.info('r2 : {:.4f} +- {:.4f}'.format(df['r2'].mean(), df['r2'].std()))\n",
    "    logging.info('pc-er : {:.4f} +- {:.4f}'.format(df['pc-er'].mean(), df['pc-er'].std()))\n",
    "    logging.info('doa : {:.4f} +- {:.4f}'.format(df['doa'].mean(), df['doa'].std()))\n",
    "    logging.info('rm : {:.4f} +- {:.4f}'.format(df['rm'].mean(), df['rm'].std()))\n",
    "\n",
    "    return metrics"
   ],
   "id": "d8a30ea4ddf5e7b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for connection file: ~/.ipython/profile_default/security/ipcontroller-client.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 13\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdill\u001B[39;00m\n\u001B[1;32m     11\u001B[0m cat_absolute_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m rc \u001B[38;5;241m=\u001B[39m Client()\n\u001B[1;32m     14\u001B[0m rc[:]\u001B[38;5;241m.\u001B[39muse_dill()\n\u001B[1;32m     15\u001B[0m lview \u001B[38;5;241m=\u001B[39m rc\u001B[38;5;241m.\u001B[39mload_balanced_view()\n",
      "File \u001B[0;32m~/anaconda3/envs/cdbpr-env/lib/python3.11/site-packages/ipyparallel/client/client.py:450\u001B[0m, in \u001B[0;36mClient.__init__\u001B[0;34m(self, connection_info, url_file, profile, profile_dir, ipython_dir, context, debug, sshserver, sshkey, password, paramiko, timeout, cluster_id, cluster, **extra_args)\u001B[0m\n\u001B[1;32m    448\u001B[0m waiting_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m waiting_time \u001B[38;5;241m<\u001B[39m timeout:\n\u001B[0;32m--> 450\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;28mmin\u001B[39m(timeout \u001B[38;5;241m-\u001B[39m waiting_time, \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    451\u001B[0m     waiting_time \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    452\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(connection_file):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T13:26:40.315069Z",
     "start_time": "2025-01-02T13:26:40.277544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "dataset_name = \"postcovid\"\n",
    "logging.info(dataset_name)\n",
    "config['learning_rate'] = 0.02026\n",
    "config['lambda'] = 1.2e-5\n",
    "config['d_in'] = 4\n",
    "config['num_responses'] = 12\n",
    "metrics = test(dataset_name,config)\n",
    "\n",
    "dataset_name = \"movielens\"\n",
    "logging.info(dataset_name)\n",
    "config['learning_rate'] = 0.02515\n",
    "config['lambda'] = 2e-7\n",
    "config['d_in'] = 10\n",
    "config['num_responses'] = 12\n",
    "metrics = test(dataset_name,config)\n",
    "\n",
    "dataset_name = \"portrait\"\n",
    "logging.info(dataset_name)\n",
    "config['learning_rate'] = 0.04568\n",
    "config['lambda'] = 2e-7\n",
    "config['d_in'] = 6\n",
    "config['num_responses'] = 12\n",
    "metrics = test(dataset_name,config)\n",
    "\n",
    "dataset_name = \"promis\"\n",
    "logging.info(dataset_name)\n",
    "config['learning_rate'] = 0.01227\n",
    "config['lambda'] = 1e-7\n",
    "config['d_in'] = 6\n",
    "config['num_responses'] = 13\n",
    "metrics = test(dataset_name,config)"
   ],
   "id": "d2aff2fe42621c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 26:40] postcovid\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:7\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'test' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.2. Sequential training and testing",
   "id": "f79e6c9c01a718e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:36:31.498261Z",
     "start_time": "2025-01-28T15:36:31.387983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "reload(utils)\n",
    "reload(model)\n",
    "reload(dataset)\n",
    "\n",
    "config[\"disable_tqdm\"] = True\n",
    "config[\"tensorboard\"] = False\n",
    "config['flush_freq'] = False\n",
    "config['early_stopping'] = True\n",
    "config['save_params']=False # Save all model parameters and save an array of the embeddings\n",
    "config['num_epochs']=200\n",
    "config['verbose_early_stopping'] = False\n",
    "config['esc'] = 'error'\n",
    "\n",
    "def test(dataset_name:str, config : dict) :\n",
    "\n",
    "    logging.info(f'#### {dataset_name} ####')\n",
    "    logging.info(f'#### config : {config} ####')\n",
    "    config['embs_path']='../embs/'+str(dataset_name)\n",
    "    config['params_path']='../ckpt/'+str(dataset_name)\n",
    "\n",
    "    metrics = {\"mae\":[],\"rmse\":[], \"r2\":[], \"pc-er\" : [], \"doa\": [], 'rm' : []}\n",
    "\n",
    "    for i_fold in range(5):\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Dataset downloading for doa and rm\n",
    "        warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in divide\")\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        concept_map = json.load(open(f'../datasets/{dataset_name}/concept_map.json', 'r'))\n",
    "        concept_map = {int(k): [int(x) for x in v] for k, v in concept_map.items()}\n",
    "        metadata = json.load(open(f'../datasets/{dataset_name}/metadata.json', 'r'))\n",
    "        concept_array, concept_lens=utils.preprocess_concept_map(concept_map)\n",
    "\n",
    "        # read datasets\n",
    "        train_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_train_quadruples_vert_{i_fold}.csv',\n",
    "                                     encoding='utf-8').to_records(index=False,\n",
    "                                                                  column_dtypes={'student_id': int, 'item_id': int,\n",
    "                                                                                 \"correct\": float,\"dimension_id\":int})\n",
    "        valid_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_valid_quadruples_vert_{i_fold}.csv',\n",
    "                                     encoding='utf-8').to_records(index=False,\n",
    "                                                                  column_dtypes={'student_id': int, 'item_id': int,\n",
    "                                                                                 \"correct\": float,\"dimension_id\":int})\n",
    "        test_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{dataset_name}_test_quadruples_vert_{i_fold}.csv',\n",
    "                                    encoding='utf-8').to_records(index=False,\n",
    "                                                                 column_dtypes={'student_id': int, 'item_id': int,\n",
    "                                                                                \"correct\": float,\"dimension_id\":int})\n",
    "\n",
    "        train_data = dataset.LoaderDataset(train_quadruplets, concept_map, metadata)\n",
    "        valid_data = dataset.LoaderDataset(valid_quadruplets, concept_map, metadata)\n",
    "        test_data = dataset.LoaderDataset(test_quadruplets, concept_map, metadata)\n",
    "\n",
    "        for seed in range(1):\n",
    "    \n",
    "            # Set the seed\n",
    "            utils.set_seed(seed)\n",
    "            config['seed'] = seed\n",
    "\n",
    "            algo = model.DBPR(**config)\n",
    "\n",
    "            # Init model\n",
    "            algo.init_model(train_data, valid_data)\n",
    "\n",
    "            # train model ----\n",
    "            algo.train(train_data, valid_data)\n",
    "\n",
    "            # test model ----\n",
    "             # test model ----\n",
    "            metrics[\"pc-er\"].append(algo.evaluate_emb(test_data,concept_map)['pc-er'])\n",
    "            eval = algo.evaluate_test(test_data)\n",
    "            metrics[\"rmse\"].append(eval[\"rmse\"].numpy())\n",
    "            metrics[\"mae\"].append(eval[\"mae\"].numpy())\n",
    "            metrics[\"r2\"].append(eval[\"r2\"].numpy())\n",
    "            emb = algo.model.users_emb.weight.detach().cpu().numpy()\n",
    "            metrics[\"doa\"].append(np.mean(utils.evaluate_doa(emb,test_data.log_tensor.cpu().numpy(),metadata,concept_map)))\n",
    "            metrics[\"rm\"].append(np.mean(utils.compute_rm_fold(emb,test_quadruplets, concept_array, concept_lens)))\n",
    "\n",
    "            pd.DataFrame(emb).to_csv(\"../embs/\"+dataset_name+\"_DBPR_cornac_Iter_fold\"+str(i_fold)+\"_seed_\"+str(seed)+\".csv\",index=False,header=False)\n",
    "\n",
    "    df = pd.DataFrame(metrics)\n",
    "    logging.info('rmse : {:.4f} +- {:.4f}'.format(df['rmse'].mean(), df['rmse'].std()))\n",
    "    logging.info('mae : {:.4f} +- {:.4f}'.format(df['mae'].mean(), df['mae'].std()))\n",
    "    logging.info('r2 : {:.4f} +- {:.4f}'.format(df['r2'].mean(), df['r2'].std()))\n",
    "    logging.info('pc-er : {:.4f} +- {:.4f}'.format(df['pc-er'].mean(), df['pc-er'].std()))\n",
    "    logging.info('doa : {:.4f} +- {:.4f}'.format(df['doa'].mean(), df['doa'].std()))\n",
    "    logging.info('rm : {:.4f} +- {:.4f}'.format(df['rm'].mean(), df['rm'].std()))\n",
    "\n",
    "    return metrics"
   ],
   "id": "59be7fecafcada09",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:36:34.472989Z",
     "start_time": "2025-01-28T15:36:33.336661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "dataset_name = \"postcovid\"\n",
    "logging.info(dataset_name)\n",
    "config['learning_rate'] = 0.02026\n",
    "config['lambda'] = 1.2e-5\n",
    "config['d_in'] = 4\n",
    "config['num_responses'] = 12\n",
    "metrics = test(dataset_name,config)\n",
    "\n",
    "dataset_name = \"movielens\"\n",
    "logging.info(dataset_name)\n",
    "config['learning_rate'] = 0.02515\n",
    "config['lambda'] = 2e-7\n",
    "config['d_in'] = 10\n",
    "config['num_responses'] = 12\n",
    "metrics = test(dataset_name,config)\n",
    "\n",
    "dataset_name = \"portrait\"\n",
    "logging.info(dataset_name)\n",
    "config['learning_rate'] = 0.04568\n",
    "config['lambda'] = 2e-7\n",
    "config['d_in'] = 6\n",
    "config['num_responses'] = 12\n",
    "metrics = test(dataset_name,config)\n",
    "\n",
    "dataset_name = \"promis\"\n",
    "logging.info(dataset_name)\n",
    "config['learning_rate'] = 0.01227\n",
    "config['lambda'] = 1e-7\n",
    "config['d_in'] = 6\n",
    "config['num_responses'] = 13\n",
    "metrics = test(dataset_name,config)"
   ],
   "id": "17019087789d375a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 36:33] postcovid\n",
      "[INFO 36:33] #### postcovid ####\n",
      "[INFO 36:33] #### config : {'seed': 0, 'load_params': False, 'save_params': False, 'embs_path': '../embs/postcovid', 'params_path': '../ckpt/postcovid', 'early_stopping': True, 'fast_training': True, 'learning_rate': 0.02026, 'batch_size': 2048, 'num_epochs': 200, 'num_dim': 10, 'eval_freq': 1, 'patience': 30, 'device': 'cuda:0', 'lambda': 1.2e-05, 'tensorboard': False, 'flush_freq': False, 'prednet_len1': 128, 'prednet_len2': 64, 'best_params_path': '', 'num_layers': 0, 'version': 'pair', 'p_dropout': 0, 'low_mem_mode': True, 'user_nbrs_n': 10, 'item_nbrs_n': 5, 'disable_tqdm': True, 'verbose_early_stopping': False, 'esc': 'error', 'd_in': 17, 'num_responses': 12} ####\n",
      "CUDA is not available. Skipping CUDA seed setting.\n",
      "CUDA is not available. Skipping CUDA seed setting.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:7\u001B[0m\n",
      "Cell \u001B[0;32mIn[12], line 67\u001B[0m, in \u001B[0;36mtest\u001B[0;34m(dataset_name, config)\u001B[0m\n\u001B[1;32m     64\u001B[0m utils\u001B[38;5;241m.\u001B[39mset_seed(seed)\n\u001B[1;32m     65\u001B[0m config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m seed\n\u001B[0;32m---> 67\u001B[0m algo \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mDBPR(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# Init model\u001B[39;00m\n\u001B[1;32m     70\u001B[0m algo\u001B[38;5;241m.\u001B[39minit_model(train_data, valid_data)\n",
      "File \u001B[0;32m~/Programmation/DBPR/CAT/model/DBPR.py:273\u001B[0m, in \u001B[0;36mDBPR.__init__\u001B[0;34m(self, **config)\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDBPR\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n\u001B[0;32m--> 273\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mL_W \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mscript(CoVWeightingLoss(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig))\n",
      "File \u001B[0;32m~/Programmation/DBPR/CAT/model/DBPR.py:37\u001B[0m, in \u001B[0;36mCoVWeightingLoss.__init__\u001B[0;34m(self, *args, **config)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Initialize tensors for online statistics\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mt: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m  \u001B[38;5;66;03m# Time step\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_L: torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_losses, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# Mean of losses\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_l: torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_losses, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# Mean of loss ratios\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mM2: torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_losses,\n\u001B[1;32m     40\u001B[0m                                     device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# Sum of squares of differences from the current mean\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/cdbpr-env/lib/python3.11/site-packages/torch/cuda/__init__.py:293\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    291\u001B[0m     )\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 293\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    296\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    297\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
